<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Inseop Chung</title>

    <meta name="author" content="Inseop Chung">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <!-- Profile Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Inseop Chung
                </p>
                <p>I'm a Staff Researcher at the AI Center of Samsung Electronics, where I work on various topics in computer vision and machine learning.
                </p>
                <p>
                  I received my PhD from <a href="https://convergence.snu.ac.kr/en/intl_inf_intro/">Graduate School of Convergence Science and Technology (GSCST), Department of Intelligence and Information</a> of <a href="https://en.snu.ac.kr">Seoul National University (SNU)</a> under the supervision of professor <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en">Nojun Kwak</a>. I was a member of the <a href="https://mipal.snu.ac.kr/mipal">Machine Intelligence and Pattern Analysis Lab (MIPAL)</a> led by Prof. Kwak. My research focuses on domain adaptation, domain generalization, test-time adaptation, object detection, semantic segmentation, knowledge distillation, and vision-language-action models.
                </p>
                <p style="text-align:center">
                  <a href="mailto:insupjung613@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=6bFY9FgAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/chung-inseop-0b3178231/?originalSubdomain=kr">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/inseopchung">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>I'm broadly interested in computer vision, deep learning, and machine learning, with a current and primary focus on Vision-Language-Action (VLA) models for embodied AI and robotics. My goal is to build AI systems that can perceive, reason, and act robustly in dynamic, real-world environments.</p>
                <p>My recent research centers on making AI models more adaptive and generalizable to unseen scenarios through techniques such as domain adaptation, domain generalization, and continual test-time adaptation (CTA). I also have experience in knowledge distillation, object detection, and semantic segmentation.</p>
                <p>Currently, I'm particularly passionate about advancing VLA models that integrate vision and language with action understanding for real-world robotic control. I actively work with open-source VLA frameworks such as OpenVLA and Physical Intelligence (OpenPI) models, and have hands-on experience in adapting and extending these platforms for robotic manipulation tasks.</p>
                <p>My long-term interest lies in developing scalable and generalizable embodied intelligence — enabling agents that can learn continuously from diverse environments, understand natural language instructions, and perform complex physical tasks with minimal supervision.</p>
              </td>
            </tr>
          </tbody></table>


          <!-- Publications Section Header -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <!-- Publications List -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="mitigating_stop()" onmouseover="mitigating_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mitigating_image'><img src='images/mitigate.jpeg' width="160"></div>
                  <img src='images/mitigate.jpeg' width="160">
                </div>
                <script type="text/javascript">
                  function mitigating_start() {
                    document.getElementById('mitigating_image').style.opacity = "1";
                  }

                  function mitigating_stop() {
                    document.getElementById('mitigating_image').style.opacity = "0";
                  }
                  mitigating_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2403.01344">
                  <span class="papertitle">Mitigating the Bias in the Model for Continual Test-Time Adaptation</span>
                </a>
                <br>
                <strong>Inseop Chung</strong>,
                Kyomin Hwang,
                Jayeon Yoo,
                Nojun Kwak
                <br>
                <em>arXiv</em>, 2024.Mar
                <br>
                <a href="https://arxiv.org/abs/2403.01344">arXiv</a>
                <p></p>
                <p>
                  This work improves Continual Test-Time Adaptation by introducing class-wise feature clustering and source-aligned prototype anchoring. It dynamically reduces prediction bias and enhances model robustness under distribution shifts.
                </p>
              </td>
            </tr>

            <tr onmouseout="open_stop()" onmouseover="open_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='open_image'><img src='images/odg.jpg' width="160"></div>
                  <img src='images/odg.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function open_start() {
                    document.getElementById('open_image').style.opacity = "1";
                  }

                  function open_stop() {
                    document.getElementById('open_image').style.opacity = "0";
                  }
                  open_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.05141">
                  <span class="papertitle">Open Domain Generalization with a Single Network by Regularization Exploiting Pretrained Features</span>
                </a>
                <br>
                <strong>Inseop Chung</strong>,
                Kiyoon Yoo,
                Nojun Kwak
                <br>
                <em>ICLR 2024 Workshop</em>, 2024.May
                <br>
                <a href="https://arxiv.org/abs/2312.05141">arXiv</a>
                <p></p>
                <p>
                  We propose a single-network method for Open Domain Generalization that improves unseen domain generalization and unknown class detection. It leverages a linear-probed head and two regularization terms to refine the classifier while preserving pre-trained features, enabling efficient and robust generalization.
                </p>
              </td>
            </tr>

            <tr onmouseout="what_stop()" onmouseover="what_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='what_image'><img src='images/cat_od.jpg' width="160"></div>
                  <img src='images/cat_od.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function what_start() {
                    document.getElementById('what_image').style.opacity = "1";
                  }

                  function what_stop() {
                    document.getElementById('what_image').style.opacity = "0";
                  }
                  what_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.08875">
                  <span class="papertitle">What, How, and When Should Object Detectors Update in Continually Changing Test Domains?</span>
                </a>
                <br>
                Jayeon Yoo,
                Dongkwan Lee,
                <strong>Inseop Chung</strong>,
                Donghyun Kim,
                Nojun Kwak
                <br>
                <em>CVPR</em>, 2024.Jun
                <br>
                <a href="https://arxiv.org/abs/2312.08875">arXiv</a>
                <p></p>
                <p>
                  We propose a lightweight, architecture-agnostic test-time adaptation method for object detection under continual domain shifts. By updating only small adaptor modules and introducing a class-wise feature alignment strategy, our approach achieves efficient, robust adaptation without modifying the backbone, improving mAP on standard benchmarks while preserving real-time performance.
                </p>
              </td>
            </tr>

            <tr onmouseout="end_stop()" onmouseover="end_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='end_image'><img src='images/e2e_od.jpg' width="160"></div>
                  <img src='images/e2e_od.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function end_start() {
                    document.getElementById('end_image').style.opacity = "1";
                  }

                  function end_stop() {
                    document.getElementById('end_image').style.opacity = "0";
                  }
                  end_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2205.08714">
                  <span class="papertitle">End-to-End Multi-Object Detection with a Regularized Mixture Model</span>
                </a>
                <br>
                Jaeyoung Yoo*,
                Hojun Lee*,
                Seunghyeon Seo,
                <strong>Inseop Chung</strong>,
                Nojun Kwak
                <br>
                <em>ICML</em>, 2023.Jul
                <br>
                <a href="https://arxiv.org/abs/2205.08714">arXiv</a>
                <p></p>
                <p>
                  We introduce D-RMM, an end-to-end multi-object detector trained with negative log-likelihood and a novel regularization loss. By modeling detection as density estimation, our method removes training heuristics, improves confidence reliability, and outperforms prior end-to-end detectors on MS COCO.
                </p>
              </td>
            </tr>

            <tr onmouseout="exploiting_stop()" onmouseover="exploiting_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='exploiting_image'><img src='images/inter_pixel_uda.jpg' width="160"></div>
                  <img src='images/inter_pixel_uda.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function exploiting_start() {
                    document.getElementById('exploiting_image').style.opacity = "1";
                  }

                  function exploiting_stop() {
                    document.getElementById('exploiting_image').style.opacity = "0";
                  }
                  exploiting_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2110.10916">
                  <span class="papertitle">Exploiting Inter-pixel Correlations in Unsupervised Domain Adaptation for Semantic Segmentation</span>
                </a>
                <br>
                <strong>Inseop Chung</strong>,
                Jayeon Yoo,
                Nojun Kwak
                <br>
                <em>WACV 2023 Workshop (Best Paper Award)</em>, 2023.Jan
                <br>
                <a href="https://arxiv.org/abs/2110.10916">arXiv</a>
                <p></p>
                <p>
                  We enhance UDA for semantic segmentation by transferring inter-pixel correlations from the source to the target domain using a self-attention module. Trained only on the source, this module guides the segmentation network on the target domain beyond noisy pseudo labels, improving performance on standard benchmarks.
                </p>
              </td>
            </tr>

            <tr onmouseout="xmas_stop()" onmouseover="xmas_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='xmas_image'><img src='images/x-mas.jpg' width="160"></div>
                  <img src='images/x-mas.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function xmas_start() {
                    document.getElementById('xmas_image').style.opacity = "1";
                  }

                  function xmas_stop() {
                    document.getElementById('xmas_image').style.opacity = "0";
                  }
                  xmas_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="#">
                  <span class="papertitle">X-MAS: Extremely Large-Scale Multi-Modal Sensor Dataset for Outdoor Surveillance in Real Environments</span>
                </a>
                <br>
                DongKi Noh,
                Chang Ki Sung,
                Taeyoung Uhm,
                Wooju Lee,
                Hyungtae Lim,
                Jaeseok Choi,
                Kyuewang Lee,
                Dasol Hong,
                Daeho Um,
                <strong>Inseop Chung</strong>,
                Hochul Shin,
                Min-Jung Kim,
                Hyoung-Rock Kim,
                Seung-Min Baek,
                Hyun Myung
                <br>
                <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2023.Jan
                <br>
                <p></p>
                <p>
                  We introduce X-MAS, a large-scale multi-modal dataset for outdoor surveillance with 500K+ annotated image pairs across five sensor types.
                </p>
              </td>
            </tr>

            <tr onmouseout="unsupervised_stop()" onmouseover="unsupervised_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='unsupervised_image'><img src='images/uda_od.jpg' width="160"></div>
                  <img src='images/uda_od.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function unsupervised_start() {
                    document.getElementById('unsupervised_image').style.opacity = "1";
                  }

                  function unsupervised_stop() {
                    document.getElementById('unsupervised_image').style.opacity = "0";
                  }
                  unsupervised_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2207.09656">
                  <span class="papertitle">Unsupervised Domain Adaptation for One-Stage Object Detector Using Offsets to the Bounding Box</span>
                </a>
                <br>
                Jayeon Yoo,
                <strong>Inseop Chung</strong>,
                Nojun Kwak
                <br>
                <em>ECCV</em>, 2022.Oct
                <br>
                <a href="https://arxiv.org/abs/2207.09656">arXiv</a>
                <p></p>
                <p>
                  We propose OADA, an offset-aware domain adaptive object detector that conditions feature alignment on bounding box offsets to reduce negative transfer. This simple yet effective method improves both discriminability and transferability, achieving state-of-the-art results in domain adaptive detection.
                </p>
              </td>
            </tr>

            <tr onmouseout="dummy_stop()" onmouseover="dummy_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dummy_image'><img src='images/osks.jpg' width="160"></div>
                  <img src='images/osks.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function dummy_start() {
                    document.getElementById('dummy_image').style.opacity = "1";
                  }

                  function dummy_stop() {
                    document.getElementById('dummy_image').style.opacity = "0";
                  }
                  dummy_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.13691">
                  <span class="papertitle">Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting</span>
                </a>
                <br>
                Byeonggeun Kim,
                Seunghan Yang,
                <strong>Inseop Chung</strong>,
                Simyung Chang
                <br>
                <em>INTERSPEECH</em>, 2022.Sep
                <br>
                <a href="https://arxiv.org/abs/2206.13691">arXiv</a>
                <p></p>
                <p>
                  We propose D-ProtoNets, a metric learning-based method for few-shot open-set keyword spotting. Using dummy prototypes, our approach effectively rejects unknown classes and outperforms recent FSOSR methods on both splitGSC and miniImageNet benchmarks.
                </p>
              </td>
            </tr>

            <tr onmouseout="personalized_stop()" onmouseover="personalized_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='personalized_image'><img src='images/pks.jpg' width="160"></div>
                  <img src='images/pks.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function personalized_start() {
                    document.getElementById('personalized_image').style.opacity = "1";
                  }

                  function personalized_stop() {
                    document.getElementById('personalized_image').style.opacity = "0";
                  }
                  personalized_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.13708">
                  <span class="papertitle">Personalized Keyword Spotting through Multi-task Learning</span>
                </a>
                <br>
                Seunghan Yang,
                Byeonggeun Kim,
                <strong>Inseop Chung</strong>,
                Simyung Chang
                <br>
                <em>INTERSPEECH</em>, 2022.Sep
                <br>
                <a href="https://arxiv.org/abs/2206.13708">arXiv</a>
                <p></p>
                <p>
                  We propose PK-MTL, a multi-task learning framework for personalized keyword spotting that jointly learns keyword spotting and speaker verification. 
                </p>
              </td>
            </tr>

            <tr onmouseout="maximizing_stop()" onmouseover="maximizing_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='maximizing_image'><img src='images/uda_ss.jpg' width="160"></div>
                  <img src='images/uda_ss.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function maximizing_start() {
                    document.getElementById('maximizing_image').style.opacity = "1";
                  }

                  function maximizing_stop() {
                    document.getElementById('maximizing_image').style.opacity = "0";
                  }
                  maximizing_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2102.13002">
                  <span class="papertitle">Maximizing Cosine Similarity Between Spatial Features for Unsupervised Domain Adaptation in Semantic Segmentation</span>
                </a>
                <br>
                <strong>Inseop Chung</strong>,
                Daesik Kim,
                Nojun Kwak
                <br>
                <em>WACV</em>, 2022.Jan
                <br>
                <a href="https://arxiv.org/abs/2102.13002">arXiv</a>
                <p></p>
                <p>
                  We present a cosine similarity-based method for unsupervised domain adaptation in semantic segmentation. By aligning source and target features via a class-wise feature dictionary and similarity maximization, our approach reduces domain gaps and improves performance on standard UDA benchmarks.
                </p>
              </td>
            </tr>

            <tr onmouseout="multi_stop()" onmouseover="multi_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='multi_image'><img src='images/iccas.jpg' width="160"></div>
                  <img src='images/iccas.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function multi_start() {
                    document.getElementById('multi_image').style.opacity = "1";
                  }

                  function multi_stop() {
                    document.getElementById('multi_image').style.opacity = "0";
                  }
                  multi_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="/">
                  <span class="papertitle">Multi-modal Object Detection, Tracking, and Action Classification for Unmanned Outdoor Surveillance Robots</span>
                </a>
                <br>
                Kyuewang Lee*,
                <strong>Inseop Chung</strong>*,
                Daeho Um,
                Jaeseok Choi,
                Yeji Song,
                Seunghyeon Seo,
                Nojun Kwak,
                Jin Young Choi
                <br>
                <em>ICCAS</em>, 2021.Oct
                <br>
                <p>
                  We present a real-time multi-modal framework for outdoor robots that detects, tracks, and classifies human actions using RGB, thermal, and LiDAR data.
                </p>
              </td>
            </tr>

            <tr onmouseout="training_stop()" onmouseover="training_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='training_image'><img src='images/mdod.jpg' width="160"></div>
                  <img src='images/mdod.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function training_start() {
                    document.getElementById('training_image').style.opacity = "1";
                  }

                  function training_stop() {
                    document.getElementById('training_image').style.opacity = "0";
                  }
                  training_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1911.12721">
                  <span class="papertitle">Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image</span>
                </a>
                <br>
                Jaeyoung Yoo,
                Hojun Lee*,
                <strong>Inseop Chung</strong>*,
                Geonseok Seo,
                Nojun Kwak
                <br>
                <em>ICCV</em>, 2021.Oct
                <br>
                <a href="https://arxiv.org/abs/1911.12721">arXiv</a>
                <p></p>
                <p>
                We propose MDOD, a novel object detector that formulates multi-object detection as bounding box density estimation using a mixture model, simplifying training and improving performance.
                </p>
              </td>
            </tr>

            <tr onmouseout="fusion_stop()" onmouseover="fusion_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='fusion_image'><img src='images/ffl.jpg' width="160"></div>
                  <img src='images/ffl.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function fusion_start() {
                    document.getElementById('fusion_image').style.opacity = "1";
                  }

                  function fusion_stop() {
                    document.getElementById('fusion_image').style.opacity = "0";
                  }
                  fusion_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/1904.09058.pdf">
                  <span class="papertitle">Feature Fusion for Online Mutual Knowledge Distillation</span>
                </a>
                <br>
                Jangho Kim,
                Minsugn Hyun,
                <strong>Inseop Chung</strong>,
                Nojun Kwak
                <br>
                <em>ICPR</em>, 2021.Jan
                <br>
                <a href="https://arxiv.org/pdf/1904.09058.pdf">arXiv</a>
                <p></p>
                <p>
                  We propose Feature Fusion Learning (FFL), a mutual knowledge distillation framework that combines features from diverse sub-networks to train a stronger fused classifier, improving both overall and individual network performance.
                </p>
              </td>
            </tr>

            <tr onmouseout="feature_stop()" onmouseover="feature_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='feature_image'><img src='images/afd.jpg' width="160"></div>
                  <img src='images/afd.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function feature_start() {
                    document.getElementById('feature_image').style.opacity = "1";
                  }

                  function feature_stop() {
                    document.getElementById('feature_image').style.opacity = "0";
                  }
                  feature_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="http://proceedings.mlr.press/v119/chung20a/chung20a.pdf">
                  <span class="papertitle">Feature-map-level Online Adversarial Knowledge Distillation</span>
                </a>
                <br>
                <strong>Inseop Chung</strong>,
                SeongUk Park,
                Jangho Kim,
                Nojun Kwak
                <br>
                <em>ICML</em>, 2020.Jul
                <br>
                <a href="http://proceedings.mlr.press/v119/chung20a/chung20a.pdf">Proceedings</a>
                <p></p>
                <p>
                  We propose an online knowledge distillation method that aligns feature maps via adversarial training, enabling networks to learn from each other's internal representations and improving performance, especially in mixed-size network setups.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Education Section -->
          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Education</h2>
                <p>
                  <strong>Ph.D.</strong> in Engineering, Seoul National University, 2019-2024<br>
                  Graduate School of Convergence Science and Technology, Department of Intelligence and Information<br>
                  Advisor: <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en">Nojun Kwak</a>
                </p>
                <p>
                  <strong>B.S.</strong> in Creative Technology Management & Computer Science, Yonsei University, 2012-2019<br>
                  Exchange Student Program at Technical University of Munich (2016-2017)
                </p>
              </td>
            </tr>
          </tbody></table>-->

          <table style="width:100%; border:0; border-spacing:0; border-collapse:separate; margin:auto;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:16px;">
                  <h2>Education</h2>
                </td>
              </tr>
          
              <!-- Ph.D. -->
              <tr>
                <td style="padding:16px; width:80px; vertical-align:top;">
                  <div style="width:70px; height:70px; display:flex; align-items:center; background-color:white;">
                    <img src="images/snu.png" alt="SNU Logo" 
                         style="width:100%; height:100%; object-fit:contain; padding:5px;">
                  </div>
                </td>
                <td style="padding:16px; vertical-align:middle;">
                  <p>
                    <strong>Ph.D.</strong> in Engineering, Seoul National University, 2019–2024<br>
                    Graduate School of Convergence Science and Technology, Department of Intelligence and Information<br>
                    Advisor: <a href="https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en">Nojun Kwak</a>
                  </p>
                </td>
              </tr>
          
              <!-- B.S. -->
              <tr>
                <td style="padding:16px; width:80px; vertical-align:top;">
                  <div style="width:70px; height:70px; display:flex; align-items:center; background-color:white;">
                    <img src="images/yonsei.jpg" alt="Yonsei Logo" 
                         style="width:100%; height:100%; object-fit:contain; padding:5px;">
                  </div>
                </td>
                <td style="padding:16px; vertical-align:middle;">
                  <p>
                    <strong>B.S.</strong> in Creative Technology Management & Computer Science, Yonsei University, 2012–2019<br>
                    Exchange Student Program at Technical University of Munich (2016–2017)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          

          <!-- Work Experience Section -->
          <!-- 
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Work Experience</h2>
                <p>
                  <strong>Staff Researcher</strong>, AI Center, Samsung Electronics, 2024.Dec.-Present<br>
                  Research on computer vision and machine learning
                </p>
                <p>
                  <strong>Staff Researcher</strong>, Samsung Advanced Institute of Technology (SAIT), Samsung Electronics, 2024.Mar-2024.Dec.<br>
                  Research on computer vision and machine learning
                </p>
                <p>
                  <strong>Staff Researcher</strong>, Samsung Electronics, 2024.Mar-Present<br>
                  As a staff researcher, I'm working on Vision-Language-Action models for robotics, focusing on enabling robots to understand and interact with their environment through multimodal sensory inputs. My work involves developing advanced models that integrate visual, linguistic, and action-based data to enhance robot perception and decision-making, driving innovations in autonomous robotics and human-robot collaboration.
                </p>
                <p>
                  <strong>Research Intern</strong>, Qualcomm AI Research Korea, 2022.Jan-2022.Jul<br>
                  As a Deep Learning Research Intern, I worked on Few-Shot Learning (FSL) for Keyword Spotting (KWS), focusing on enabling models to recognize new keywords with minimal labeled examples. My contributions included developing and optimizing few-shot learning algorithms to improve robustness and efficiency in speech recognition tasks.
                </p>
                <p>
                  <strong>Research Intern</strong>, Naver Webtoons Corp., 2020.Jun-2020.Dec<br>
                  As a Deep Learning Research Intern, I worked on Unsupervised Domain Adaptation (UDA) for semantic segmentation, aiming to improve model generalization across different domains. My contributions included developing novel adaptation techniques to mitigate domain shifts and enhancing segmentation accuracy in diverse webtoon styles. Through this work, I gained hands-on experience in domain adaptation strategies and their practical applications in real-world datasets.
                </p>
              </td>
            </tr>
          </tbody></table>-->

          
          <table style="width:100%; border:0; border-spacing:0; border-collapse:separate; margin:auto;">
            <tbody>
              <tr>
                <td colspan="2" style="padding:16px;">
                  <h2>Work Experience</h2>
                </td>
              </tr>

              <!-- Samsung -->
              <tr>
                <td style="padding:16px; width:80px; vertical-align:top;">
                  <div style="width:70px; height:70px; display:flex; align-items:center; justify-content:center;">
                    <img src="images/samsung.png" alt="Samsung Logo" style="max-width:100%; max-height:100%; object-fit:contain;">
                  </div>
                </td>
                <td style="padding:16px; vertical-align:middle;">
                  <p>
                    <strong>Staff Researcher</strong>, Samsung Electronics, 2024.Mar–Present<br>
                    Working on Vision-Language-Action models for robotics, integrating visual, linguistic, and action-based inputs to enhance perception and decision-making in autonomous systems.
                  </p>
                </td>
              </tr>

              <!-- Qualcomm -->
              <tr>
                <td style="padding:16px; width:80px; vertical-align:top;">
                  <div style="width:70px; height:70px; display:flex; align-items:center; justify-content:center;">
                    <img src="images/qualcomm.png" alt="Qualcomm Logo" style="max-width:100%; max-height:100%; object-fit:contain;">
                  </div>
                </td>
                <td style="padding:16px; vertical-align:middle;">
                  <p>
                    <strong>Research Intern</strong>, Qualcomm AI Research Korea, 2022.Jan–2022.Jul<br>
                    Developed few-shot learning methods for keyword spotting to recognize new keywords from minimal examples, enhancing speech recognition performance.
                  </p>
                </td>
              </tr>

              <!-- Naver Webtoon -->
              <tr>
                <td style="padding:16px; width:80px; vertical-align:top;">
                  <div style="width:60px; height:60px; display:flex; align-items:center; justify-content:center;">
                    <img src="images/webtoon.png" alt="Naver Webtoon Logo" style="max-width:100%; max-height:100%; object-fit:contain;">
                  </div>
                </td>
                <td style="padding:16px; vertical-align:middle;">
                  <p>
                    <strong>Research Intern</strong>, Naver Webtoons Corp., 2020.Jun–2020.Dec<br>
                    Researched unsupervised domain adaptation for semantic segmentation, improving model generalization across diverse webtoon styles through novel adaptation strategies.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>



          <!-- Patents Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Patents</h2>
                <p>
                  <strong>Domestic Patents</strong>
                </p>
                <ul>
                  <li>Device for Unsupervised Domain Adaptation in Semantic Segmentation Exploiting Inter-pixel Correlations and Driving Method Thereof</li>
                  <br>
                  <li>Device for Regression Scale-aware Cross-domain Object Detection and Driving Method Thereof</li>
                </ul>
                <p>
                  <strong>International Patents</strong>
                </p>
                <ul>
                  <li>Method and Apparatus with Online Task Planning</li>
                  <br>
                  <li>Method and Apparatus with Micro-Action Determination</li>
                  <br>
                  <li>Multi-Task Learning for For Personalized Keyword Spotting</li>
                  <br>
                  <li>Dummy Prototypical Networks For Few-Shot Open-Set Keyword Spotting</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <!-- Funding Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Funding</h2>
                <ul>
                  <li>Samsung Electronics Ph.D. Student Sponsorship Program (2021-2024)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <!-- Honour & Awards Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Honour & Awards</h2>
                <ul>
                  <li>The best paper award in the 3rd Workshop on Real-World Surveillance: Applications and Challenges at WACV2023, $1,000 (2023)</li>
                  <br>
                  <li>BK21 Graduate School Innovation Project Colloquium Outstanding Graduate Student Award, ￦500,000 (2021)</li>
                  <br>
                  <li>Qualcomm Innovation Fellowship Korea 2020 Final Session, Finalist (2020)</li>
                  <br>
                  <li>Samsung Electronics - Seoul National University, Industry-Academic Cooperation Research Award, ￦1,500,000 Won (2020)</li>
                  <br>
                  <li>Special Award, Yonsei Programming Contest 2018 (Spring Semester 2018)</li>
                  <br>
                  <li>Honours, tuition support scholarship, Yonsei University (Spring Semester 2016)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

    <!-- Acknowledgment Section -->
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:16px;width:100%;vertical-align:middle;text-align:center;font-size:14px;color:#666;">
          <p>Website template courtesy of <a href="https://jonbarron.info/">Jon Barron</a></p>
        </td>
      </tr>
    </tbody></table>
  </body>
</html>
